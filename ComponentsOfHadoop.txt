Components of Hadoop 2.x
1. Hadoop Common:
-----------------
Hadoop Common is the collection of common utilities and libraries that support other Hadoop modules. 
It is the base/core of the framework and it provides essential services and basic processes such as abstraction of the underlying operating system and its file system. Hadoop Common also contains the necessary Java Archive (JAR) files and scripts required to start Hadoop. All other components work on top of this component.
2. HDFS:
--------
The Hadoop Distributed File System (HDFS) is a distributed file system that runs on standard or low-end hardware. The HDFS stores a large amount of data placed across multiple machines. The HDFS architecture consists of clusters, each of which is accessed through a single NameNode software tool installed on a separate machine to monitor and manage the that cluster's file system and user access mechanism. The other machines install one instance of DataNode to manage cluster storage. 
HDFS has 3 main components
2.1 NameNode:
-------------
It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. NameNode holds the meta-data for the HDFS like Namespace information, block information etc. but does not store the data of these files itself. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. 
2.2 DataNode:
-------------
A DataNode stores the actual data in the HadoopFileSystem. A HDFS filesystem has more than one DataNode, with data replicated across them. The DataNodes are responsible for serving read and write requests from the file systemâ€™s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode
2.3 Secondary NameNode:
-----------------------
It is an optional component that can be hosted on a separate machine. It only creates checkpoints of the namespace by merging the edits file into the fsimage file and does not provide any real redundancy.

3 MapReduce:
------------
MapReduce is a programming model which is used to process large data sets in a batch processing manner or Distributed Data Processing Module. It is a Java-based system where the actual data from the HDFS store gets processed efficiently. MapReduce breaks down a big data processing job into smaller tasks. MapReduce is responsible for the analyzing large datasets in parallel before reducing it to find the results.
MapReduce consists of 2 phases:
1. Map() procedure that performs filtering and sorting (such as sorting students by first name into queues, one queue for each name)
2. Reduce() procedure that performs a summary operation (such as counting the number of students in each queue, yielding name frequencies).

4.	YARN
---------
Apache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology  - a clustering platform that helps to manage resources and schedule tasks.  It is a software rewrite that decouples MapReduce's resource management and scheduling capabilities from the data processing component, enabling Hadoop to support more varied processing approaches and a broader array of applications.




	